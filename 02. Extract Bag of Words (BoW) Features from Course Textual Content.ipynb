{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork32585014-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Extract Bag of Words (BoW) Features from Course Textual Content**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **60** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of recommender systems is to help users find items they potentially interested in. Depending on the recommendation tasks, an item can be a movie, a restaurant, or, in our case, an online course.\n",
    "\n",
    "Machine learning algorithms cannot work on an item directly so we first need to extract features and represent the items mathematically, i.e., with a feature vector.\n",
    "\n",
    "Many items are often described by text so they are associated with textual data, such as the titles and descriptions of a movie or course. Since machine learning algorithms can not process textual data directly, we need to transform the raw text into numeric feature vectors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module\\_2/images/extract_textual_features.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will be learning to extract the bag of words (BoW) features from course titles and descriptions. The BoW feature is a simple but effective feature characterizing textual data and is widely used in many textual machine learning tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Extract Bag of Words (BoW) features from course titles and descriptions\n",
    "*   Build a course BoW dataset to be used for building a content-based recommender system later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and setup the lab environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's install and import required libraries:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.6.7\n",
      "  Downloading nltk-3.6.7-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib in c:\\python310\\lib\\site-packages (from nltk==3.6.7) (1.2.0)\n",
      "Requirement already satisfied: click in c:\\python310\\lib\\site-packages (from nltk==3.6.7) (8.1.3)\n",
      "Requirement already satisfied: tqdm in c:\\python310\\lib\\site-packages (from nltk==3.6.7) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\python310\\lib\\site-packages (from nltk==3.6.7) (2022.10.31)\n",
      "Requirement already satisfied: colorama in c:\\python310\\lib\\site-packages (from click->nltk==3.6.7) (0.4.6)\n",
      "Installing collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.8.1\n",
      "    Uninstalling nltk-3.8.1:\n",
      "      Successfully uninstalled nltk-3.8.1\n",
      "Successfully installed nltk-3.6.7\n",
      "Collecting gensim==4.1.2\n",
      "  Downloading gensim-4.1.2.tar.gz (23.2 MB)\n",
      "     ---------------------------------------- 23.2/23.2 MB 1.9 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\python310\\lib\\site-packages (from gensim==4.1.2) (1.23.4)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\python310\\lib\\site-packages (from gensim==4.1.2) (1.9.3)\n",
      "Collecting smart_open>=1.8.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "     -------------------------------------- 56.8/56.8 kB 989.7 kB/s eta 0:00:00\n",
      "Building wheels for collected packages: gensim\n",
      "  Building wheel for gensim (setup.py): started\n",
      "  Building wheel for gensim (setup.py): finished with status 'done'\n",
      "  Created wheel for gensim: filename=gensim-4.1.2-cp310-cp310-win_amd64.whl size=23910668 sha256=f00ee311305a6f35fa48173191c86610d47d5a11178dc379301002358698bdc9\n",
      "  Stored in directory: c:\\users\\marco\\appdata\\local\\pip\\cache\\wheels\\13\\35\\4e\\dca2954de21981d0a137ff930239f0767403a617e32f19f04f\n",
      "Successfully built gensim\n",
      "Installing collected packages: smart_open, gensim\n",
      "Successfully installed gensim-4.1.2 smart_open-6.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.6.7\n",
    "!pip install gensim==4.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim import corpora\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Marco\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# also set a random state\n",
    "rs = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW) features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BoW features are essentially the counts or frequencies of each word that appears in a text (string). Let's illustrate it with some simple examples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have two course descriptions as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "course1 = \"this is an introduction data science course which introduces data science to beginners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "course2 = \"machine learning for beginners\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is an introduction data science course which introduces data science to beginners',\n",
       " 'machine learning for beginners']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses = [course1, course2]\n",
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to split the two strings into words (tokens). A token in the text processing context means the smallest unit of text such as a word, a symbol/punctuation, or a phrase, etc. The process to transform a string into a collection of tokens is called `tokenization`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common way to do `tokenization` is to use the Python built-in `split()` method of the `str` class.  However, in this lab, we want to leverage the `nltk` (Natural Language Toolkit) package, which is probably the most commonly used package to process text or natural language.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More specifically, we will use the `word_tokenize()` method on the content of course (string):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the two courses\n",
    "tokenized_courses = [word_tokenize(course) for course in courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this',\n",
       "  'is',\n",
       "  'an',\n",
       "  'introduction',\n",
       "  'data',\n",
       "  'science',\n",
       "  'course',\n",
       "  'which',\n",
       "  'introduces',\n",
       "  'data',\n",
       "  'science',\n",
       "  'to',\n",
       "  'beginners'],\n",
       " ['machine', 'learning', 'for', 'beginners']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_courses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the cell output, two courses have been tokenized and turned into two token arrays.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to create a token dictionary to index all tokens. Basically, we want to assign a key/index for each token. One way to index tokens is to use the `gensim` package which is another popular package for processing textual data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a token dictionary for the two courses\n",
    "tokens_dict = gensim.corpora.Dictionary(tokenized_courses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'an': 0, 'beginners': 1, 'course': 2, 'data': 3, 'introduces': 4, 'introduction': 5, 'is': 6, 'science': 7, 'this': 8, 'to': 9, 'which': 10, 'for': 11, 'learning': 12, 'machine': 13}\n"
     ]
    }
   ],
   "source": [
    "print(tokens_dict.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the token dictionary, we can easily count each token in the two example courses and output two BoW feature vectors. However, more conveniently, the `gensim` package provides us a `doc2bow` method to generate BoW features out-of-box.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate BoW features for each course\n",
    "courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1),\n",
       "  (1, 1),\n",
       "  (2, 1),\n",
       "  (3, 2),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 2),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 1)],\n",
       " [(1, 1), (11, 1), (12, 1), (13, 1)]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs two BoW arrays where each element is a tuple, e.g., (0, 1) and (7, 2). The first element of the tuple is the token ID and the second element is its count. So `(0, 1)` means `(``an``, 1)` and `(7, 2)` means `(``science``, 2)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the following code snippet to print each token and its count:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of words for course 0:\n",
      "--Token: 'an', Count:1\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'course', Count:1\n",
      "--Token: 'data', Count:2\n",
      "--Token: 'introduces', Count:1\n",
      "--Token: 'introduction', Count:1\n",
      "--Token: 'is', Count:1\n",
      "--Token: 'science', Count:2\n",
      "--Token: 'this', Count:1\n",
      "--Token: 'to', Count:1\n",
      "--Token: 'which', Count:1\n",
      "Bag of words for course 1:\n",
      "--Token: 'beginners', Count:1\n",
      "--Token: 'for', Count:1\n",
      "--Token: 'learning', Count:1\n",
      "--Token: 'machine', Count:1\n"
     ]
    }
   ],
   "source": [
    "for course_idx, course_bow in enumerate(courses_bow):\n",
    "    print(f\"Bag of words for course {course_idx}:\")\n",
    "    # For each token index, print its bow value (word count)\n",
    "    for token_index, token_bow in course_bow:\n",
    "        token = tokens_dict.get(token_index)\n",
    "        print(f\"--Token: '{token}', Count:{token_bow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we turn to the long list into a horizontal feature vectors, we can see the two courses become two numerical feature vectors:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module\\_2/images/bow.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BoW dimensionality reduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A document may contain tens of thousands of words which makes the dimension of the BoW feature vector huge. To reduce the dimensionality, one common way is to filter the relatively meaningless tokens such as stop words or sometimes add position and adjective words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note there are many other ways to reduce dimensionality such as `stemming` and `lemmatization` but they are beyond the scope of this capstone project. You are encouraged to explore them yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the english stop words provided in `nltk`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can filter those English stop words from the tokens in course1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'which',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'to',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokens in course 1\n",
    "tokenized_courses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_tokens = [w for w in tokenized_courses[0] if not w.lower() in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['introduction',\n",
       " 'data',\n",
       " 'science',\n",
       " 'course',\n",
       " 'introduces',\n",
       " 'data',\n",
       " 'science',\n",
       " 'beginners']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the number of tokens for `course1` has been reduced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way is to only keep nouns in the text. We can use the `nltk.pos_tag()` method to analyze the part of speech (POS) and annotate each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('introduction', 'NN'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('course', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('introduces', 'VBZ'),\n",
       " ('data', 'NNS'),\n",
       " ('science', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('beginners', 'NNS')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = nltk.pos_tag(tokenized_courses[0])\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see \\[`introduction`, `data`, `science`, `course`, `beginners`] are all of the nouns and we may keep them in the BoW feature vector.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TASK: Extract BoW features for course textual content and build a dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you have learned what a BoW feature is, so let's start extracting BoW features from some real course textual content.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/course_processed.csv\"\n",
    "course_content_df = pd.read_csv(course_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COURSE_ID                                               ML0201EN\n",
       "TITLE          robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION    have fun with iot and learn along the way  if ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The course content dataset has three columns `COURSE_ID`, `TITLE`, and `DESCRIPTION`. `TITLE` and `DESCRIPTION` are all text upon which we want to extract BoW features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's join those two text columns together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge TITLE and DESCRIPTION title\n",
    "course_content_df['course_texts'] = course_content_df[['TITLE', 'DESCRIPTION']].agg(' '.join, axis=1)\n",
    "course_content_df = course_content_df.reset_index()\n",
    "course_content_df['index'] = course_content_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                           0\n",
       "COURSE_ID                                                ML0201EN\n",
       "TITLE           robots are coming  build iot apps with watson ...\n",
       "DESCRIPTION     have fun with iot and learn along the way  if ...\n",
       "course_texts    robots are coming  build iot apps with watson ...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we have prepared a `tokenize_course()` method for you to tokenize the course content:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_course(course, keep_only_nouns=True):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(course)\n",
    "    # Remove English stop words and numbers\n",
    "    word_tokens = [w for w in word_tokens if (not w.lower() in stop_words) and (not w.isnumeric())]\n",
    "    # Only keep nouns \n",
    "    if keep_only_nouns:\n",
    "        filter_list = ['WDT', 'WP', 'WRB', 'FW', 'IN', 'JJR', 'JJS', 'MD', 'PDT', 'POS', 'PRP', 'RB', 'RBR', 'RBS',\n",
    "                       'RP']\n",
    "        tags = nltk.pos_tag(word_tokens)\n",
    "        word_tokens = [word for word, pos in tags if pos not in filter_list]\n",
    "\n",
    "    return word_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it on the first course.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'robots are coming  build iot apps with watson  swift  and node red have fun with iot and learn along the way  if you re a swift developer and want to learn more about iot and watson ai services in the cloud  raspberry pi   and node red  you ve found the right place  you ll build iot apps to read temperature data  take pictures with a raspcam  use ai to recognize the objects in those pictures  and program an irobot create 2 robot  '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_course = course_content_df.iloc[0, :]['course_texts']\n",
    "a_course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['robots',\n",
       " 'coming',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'watson',\n",
       " 'swift',\n",
       " 'red',\n",
       " 'fun',\n",
       " 'iot',\n",
       " 'learn',\n",
       " 'way',\n",
       " 'swift',\n",
       " 'developer',\n",
       " 'want',\n",
       " 'learn',\n",
       " 'iot',\n",
       " 'watson',\n",
       " 'ai',\n",
       " 'services',\n",
       " 'cloud',\n",
       " 'raspberry',\n",
       " 'pi',\n",
       " 'node',\n",
       " 'red',\n",
       " 'found',\n",
       " 'place',\n",
       " 'build',\n",
       " 'iot',\n",
       " 'apps',\n",
       " 'read',\n",
       " 'temperature',\n",
       " 'data',\n",
       " 'take',\n",
       " 'pictures',\n",
       " 'raspcam',\n",
       " 'use',\n",
       " 'ai',\n",
       " 'recognize',\n",
       " 'objects',\n",
       " 'pictures',\n",
       " 'program',\n",
       " 'irobot',\n",
       " 'create',\n",
       " 'robot']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_course(a_course)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will need to write some code snippets to generate the BoW features for each course. Let's start by tokenzing all courses in the `courses_df`:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Use provided tokenize_course() method to tokenize all courses in course_content_df\\['course_texts'].*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['robots',\n",
       "  'coming',\n",
       "  'build',\n",
       "  'iot',\n",
       "  'apps',\n",
       "  'watson',\n",
       "  'swift',\n",
       "  'red',\n",
       "  'fun',\n",
       "  'iot',\n",
       "  'learn',\n",
       "  'way',\n",
       "  'swift',\n",
       "  'developer',\n",
       "  'want',\n",
       "  'learn',\n",
       "  'iot',\n",
       "  'watson',\n",
       "  'ai',\n",
       "  'services',\n",
       "  'cloud',\n",
       "  'raspberry',\n",
       "  'pi',\n",
       "  'node',\n",
       "  'red',\n",
       "  'found',\n",
       "  'place',\n",
       "  'build',\n",
       "  'iot',\n",
       "  'apps',\n",
       "  'read',\n",
       "  'temperature',\n",
       "  'data',\n",
       "  'take',\n",
       "  'pictures',\n",
       "  'raspcam',\n",
       "  'use',\n",
       "  'ai',\n",
       "  'recognize',\n",
       "  'objects',\n",
       "  'pictures',\n",
       "  'program',\n",
       "  'irobot',\n",
       "  'create',\n",
       "  'robot']]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "tokenized_courses = [tokenize_course(course_text) for course_text in course_content_df['course_texts']]\n",
    "tokenized_courses[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to create a token dictionary `tokens_dict`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Use gensim.corpora.Dictionary(tokenized_courses) to create a token dictionary.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ai': 0,\n",
       " 'apps': 1,\n",
       " 'build': 2,\n",
       " 'cloud': 3,\n",
       " 'coming': 4,\n",
       " 'create': 5,\n",
       " 'data': 6,\n",
       " 'developer': 7,\n",
       " 'found': 8,\n",
       " 'fun': 9,\n",
       " 'iot': 10,\n",
       " 'irobot': 11,\n",
       " 'learn': 12,\n",
       " 'node': 13,\n",
       " 'objects': 14,\n",
       " 'pi': 15,\n",
       " 'pictures': 16,\n",
       " 'place': 17,\n",
       " 'program': 18,\n",
       " 'raspberry': 19,\n",
       " 'raspcam': 20,\n",
       " 'read': 21,\n",
       " 'recognize': 22,\n",
       " 'red': 23,\n",
       " 'robot': 24,\n",
       " 'robots': 25,\n",
       " 'services': 26,\n",
       " 'swift': 27,\n",
       " 'take': 28,\n",
       " 'temperature': 29,\n",
       " 'use': 30,\n",
       " 'want': 31,\n",
       " 'watson': 32,\n",
       " 'way': 33,\n",
       " 'accelerate': 34,\n",
       " 'accelerated': 35,\n",
       " 'accelerating': 36,\n",
       " 'analyze': 37,\n",
       " 'based': 38,\n",
       " 'benefit': 39,\n",
       " 'caffe': 40,\n",
       " 'case': 41,\n",
       " 'chips': 42,\n",
       " 'classification': 43,\n",
       " 'comfortable': 44,\n",
       " 'complex': 45,\n",
       " 'computations': 46,\n",
       " 'convolutional': 47,\n",
       " 'course': 48,\n",
       " 'datasets': 49,\n",
       " 'deep': 50,\n",
       " 'dependencies': 51,\n",
       " 'deploy': 52,\n",
       " 'designed': 53,\n",
       " 'feel': 54,\n",
       " 'google': 55,\n",
       " 'gpu': 56,\n",
       " 'hardware': 57,\n",
       " 'house': 58,\n",
       " 'ibm': 59,\n",
       " 'images': 60,\n",
       " 'including': 61,\n",
       " 'inference': 62,\n",
       " 'large': 63,\n",
       " 'learning': 64,\n",
       " 'libraries': 65,\n",
       " 'machine': 66,\n",
       " 'models': 67,\n",
       " 'need': 68,\n",
       " 'needs': 69,\n",
       " 'network': 70,\n",
       " 'networks': 71,\n",
       " 'neural': 72,\n",
       " 'nvidia': 73,\n",
       " 'one': 74,\n",
       " 'overcome': 75,\n",
       " 'platform': 76,\n",
       " 'popular': 77,\n",
       " 'power': 78,\n",
       " 'preferring': 79,\n",
       " 'premise': 80,\n",
       " 'problem': 81,\n",
       " 'problems': 82,\n",
       " 'processing': 83,\n",
       " 'public': 84,\n",
       " 'reduce': 85,\n",
       " 'scalability': 86,\n",
       " 'scaling': 87,\n",
       " 'sensitiveand': 88,\n",
       " 'several': 89,\n",
       " 'solution': 90,\n",
       " 'support': 91,\n",
       " 'supports': 92,\n",
       " 'system': 93,\n",
       " 'systems': 94,\n",
       " 'takes': 95,\n",
       " 'tensor': 96,\n",
       " 'tensorflow': 97,\n",
       " 'theano': 98,\n",
       " 'time': 99,\n",
       " 'torch': 100,\n",
       " 'tpu': 101,\n",
       " 'trained': 102,\n",
       " 'training': 103,\n",
       " 'understand': 104,\n",
       " 'unit': 105,\n",
       " 'uploading': 106,\n",
       " 'videos': 107,\n",
       " 'client': 108,\n",
       " 'consuming': 109,\n",
       " 'http': 110,\n",
       " 'invoke': 111,\n",
       " 'jax': 112,\n",
       " 'microservices': 113,\n",
       " 'reactive': 114,\n",
       " 'restful': 115,\n",
       " 'rs': 116,\n",
       " 'using': 117,\n",
       " 'analysis': 118,\n",
       " 'analyzing': 119,\n",
       " 'apache': 120,\n",
       " 'api': 121,\n",
       " 'big': 122,\n",
       " 'cluster': 123,\n",
       " 'computing': 124,\n",
       " 'distributed': 125,\n",
       " 'enables': 126,\n",
       " 'familiar': 127,\n",
       " 'frame': 128,\n",
       " 'framework': 129,\n",
       " 'performing': 130,\n",
       " 'provides': 131,\n",
       " 'r': 132,\n",
       " 'scale': 133,\n",
       " 'spark': 134,\n",
       " 'sparkr': 135,\n",
       " 'structured': 136,\n",
       " 'syntax': 137,\n",
       " 'used': 138,\n",
       " 'users': 139,\n",
       " 'application': 140,\n",
       " 'boot': 141,\n",
       " 'containerize': 142,\n",
       " 'containerizing': 143,\n",
       " 'liberty': 144,\n",
       " 'modification': 145,\n",
       " 'open': 146,\n",
       " 'package': 147,\n",
       " 'packaging': 148,\n",
       " 'run': 149,\n",
       " 'running': 150,\n",
       " 'server': 151,\n",
       " 'spring': 152,\n",
       " 'conference': 153,\n",
       " 'introduction': 154,\n",
       " 'native': 155,\n",
       " 'security': 156,\n",
       " 'bootcamp': 157,\n",
       " 'day': 158,\n",
       " 'intensive': 159,\n",
       " 'multi': 160,\n",
       " 'offered': 161,\n",
       " 'person': 162,\n",
       " 'proffesors': 163,\n",
       " 'science': 164,\n",
       " 'university': 165,\n",
       " 'containers': 166,\n",
       " 'development': 167,\n",
       " 'docker': 168,\n",
       " 'iterative': 169,\n",
       " 'scorm': 170,\n",
       " 'scron': 171,\n",
       " 'test': 172,\n",
       " 'basic': 173,\n",
       " 'collections': 174,\n",
       " 'creating': 175,\n",
       " 'database': 176,\n",
       " 'document': 177,\n",
       " 'first': 178,\n",
       " 'get': 179,\n",
       " 'guided': 180,\n",
       " 'management': 181,\n",
       " 'mongodb': 182,\n",
       " 'project': 183,\n",
       " 'started': 184,\n",
       " 'working': 185,\n",
       " 'arquillian': 186,\n",
       " 'container': 187,\n",
       " 'develop': 188,\n",
       " 'managed': 189,\n",
       " 'testing': 190,\n",
       " 'tests': 191,\n",
       " 'aiops': 192,\n",
       " 'attending': 193,\n",
       " 'comprehensive': 194,\n",
       " 'demonstrate': 195,\n",
       " 'digital': 196,\n",
       " 'essentials': 197,\n",
       " 'hands': 198,\n",
       " 'integration': 199,\n",
       " 'pak': 200,\n",
       " 'received': 201,\n",
       " 'short': 202,\n",
       " 'analytics': 203,\n",
       " 'assemble': 204,\n",
       " 'base': 205,\n",
       " 'basics': 206,\n",
       " 'dataset': 207,\n",
       " 'fundamentals': 208,\n",
       " 'improve': 209,\n",
       " 'line': 210,\n",
       " 'media': 211,\n",
       " 'refine': 212,\n",
       " 'reports': 213,\n",
       " 'service': 214,\n",
       " 'setup': 215,\n",
       " 'smart': 216,\n",
       " 'social': 217,\n",
       " 'suggestions': 218,\n",
       " 'teaches': 219,\n",
       " 'topic': 220,\n",
       " 'advance': 221,\n",
       " 'professors': 222,\n",
       " 'python': 223,\n",
       " 'algorithm': 224,\n",
       " 'algorithmic': 225,\n",
       " 'cryptocurrency': 226,\n",
       " 'dive': 227,\n",
       " 'earning': 228,\n",
       " 'good': 229,\n",
       " 'money': 230,\n",
       " 'right': 231,\n",
       " 'sleep': 232,\n",
       " 'sound': 233,\n",
       " 'trading': 234,\n",
       " 'true': 235,\n",
       " 'world': 236,\n",
       " 'access': 237,\n",
       " 'control': 238,\n",
       " 'eclipse': 239,\n",
       " 'java': 240,\n",
       " 'json': 241,\n",
       " 'jwt': 242,\n",
       " 'microprofile': 243,\n",
       " 'role': 244,\n",
       " 'securing': 245,\n",
       " 'token': 246,\n",
       " 'user': 247,\n",
       " 'web': 248,\n",
       " 'customize': 249,\n",
       " 'enable': 250,\n",
       " 'enabling': 251,\n",
       " 'methods': 252,\n",
       " 'non': 253,\n",
       " 'opentracing': 254,\n",
       " 'tracing': 255,\n",
       " 'zipkin': 256,\n",
       " 'another': 257,\n",
       " 'applying': 258,\n",
       " 'architecture': 259,\n",
       " 'chance': 260,\n",
       " 'common': 261,\n",
       " 'connect': 262,\n",
       " 'cover': 263,\n",
       " 'curve': 264,\n",
       " 'different': 265,\n",
       " 'easy': 266,\n",
       " 'exercises': 267,\n",
       " 'existing': 268,\n",
       " 'explain': 269,\n",
       " 'file': 270,\n",
       " 'formats': 271,\n",
       " 'hadoop': 272,\n",
       " 'help': 273,\n",
       " 'lab': 274,\n",
       " 'list': 275,\n",
       " 'load': 276,\n",
       " 'orc': 277,\n",
       " 'parquet': 278,\n",
       " 'proprietary': 279,\n",
       " 'relational': 280,\n",
       " 'sample': 281,\n",
       " 'schemas': 282,\n",
       " 'see': 283,\n",
       " 'show': 284,\n",
       " 'sql': 285,\n",
       " 'storage': 286,\n",
       " 'supported': 287,\n",
       " 'table': 288,\n",
       " 'tool': 289,\n",
       " 'types': 290,\n",
       " 'work': 291,\n",
       " '‚äì': 292,\n",
       " 'hybrid': 293,\n",
       " 'pipelines': 294,\n",
       " 'dataops': 295,\n",
       " 'methodology': 296,\n",
       " 'ops': 297,\n",
       " 'introduce': 298,\n",
       " 'journey': 299,\n",
       " 'jumpstart': 300,\n",
       " 'concepts': 301,\n",
       " 'contribute': 302,\n",
       " 'introduces': 303,\n",
       " 'key': 304,\n",
       " 'processes': 305,\n",
       " 'software': 306,\n",
       " 'source': 307,\n",
       " 'tools': 308,\n",
       " 'cloudpak': 309,\n",
       " 'end': 310,\n",
       " 'applications': 311,\n",
       " 'artificial': 312,\n",
       " 'everyone': 313,\n",
       " 'intelligence': 314,\n",
       " 'master': 315,\n",
       " 'understanding': 316,\n",
       " 'composed': 317,\n",
       " 'coupled': 318,\n",
       " 'functions': 319,\n",
       " 'include': 320,\n",
       " 'microservice': 321,\n",
       " 'serverless': 322,\n",
       " 'teach': 323,\n",
       " 'customer': 324,\n",
       " 'moddels': 325,\n",
       " 'predict': 326,\n",
       " 'predicting': 327,\n",
       " 'satisfaction': 328,\n",
       " 'satisfactions': 329,\n",
       " 'bean': 330,\n",
       " 'constraints': 331,\n",
       " 'input': 332,\n",
       " 'javabeans': 333,\n",
       " 'validate': 334,\n",
       " 'validating': 335,\n",
       " 'validation': 336,\n",
       " 'algorithms': 337,\n",
       " 'alternative': 338,\n",
       " 'amounts': 339,\n",
       " 'apis': 340,\n",
       " 'built': 341,\n",
       " 'cassandra': 342,\n",
       " 'combines': 343,\n",
       " 'diverse': 344,\n",
       " 'ease': 345,\n",
       " 'engine': 346,\n",
       " 'getting': 347,\n",
       " 'handle': 348,\n",
       " 'hbase': 349,\n",
       " 'hdfs': 350,\n",
       " 'interactive': 351,\n",
       " 'latency': 352,\n",
       " 'lightning': 353,\n",
       " 'low': 354,\n",
       " 'makers': 355,\n",
       " 'map': 356,\n",
       " 'memory': 357,\n",
       " 'mesos': 358,\n",
       " 'mining': 359,\n",
       " 'performs': 360,\n",
       " 'range': 361,\n",
       " 'requires': 362,\n",
       " 'runs': 363,\n",
       " 's3': 364,\n",
       " 'scala': 365,\n",
       " 'scenarios': 366,\n",
       " 'sources': 367,\n",
       " 'speed': 368,\n",
       " 'speeds': 369,\n",
       " 'standalone': 370,\n",
       " 'streaming': 371,\n",
       " 'times': 372,\n",
       " 'top': 373,\n",
       " 'typical': 374,\n",
       " 'wide': 375,\n",
       " 'company': 376,\n",
       " 'financial': 377,\n",
       " 'performance': 378,\n",
       " 'db2': 379,\n",
       " 'clustering': 380,\n",
       " 'investment': 381,\n",
       " 'portfolio': 382,\n",
       " 'creation': 383,\n",
       " 'lamp': 384,\n",
       " 'linux': 385,\n",
       " 'mysql': 386,\n",
       " 'php': 387,\n",
       " 'scripting': 388,\n",
       " 'stack': 389,\n",
       " 'tutorial': 390,\n",
       " 'ubuntu': 391,\n",
       " 'virtual': 392,\n",
       " 'walks': 393,\n",
       " 'game': 394,\n",
       " 'javascript': 395,\n",
       " 'language': 396,\n",
       " 'paper': 397,\n",
       " 'programming': 398,\n",
       " 'recreating': 399,\n",
       " 'rock': 400,\n",
       " 'scissors': 401,\n",
       " 'databases': 402,\n",
       " 'intent': 403,\n",
       " 'magic': 404,\n",
       " 'modify': 405,\n",
       " 'perform': 406,\n",
       " 'query': 407,\n",
       " 'simple': 408,\n",
       " 'unlock': 409,\n",
       " 'update': 410,\n",
       " 'visualizations': 411,\n",
       " 'acceleration': 412,\n",
       " 'blu': 413,\n",
       " 'c': 414,\n",
       " 'environment': 415,\n",
       " 'express': 416,\n",
       " 'focus': 417,\n",
       " 'interface': 418,\n",
       " 'mangement': 419,\n",
       " 'rdbms': 420,\n",
       " 'rodbc': 421,\n",
       " 'stored': 422,\n",
       " 'building': 423,\n",
       " 'critical': 424,\n",
       " 'exposes': 425,\n",
       " 'field': 426,\n",
       " 'foundational': 427,\n",
       " 'ii': 428,\n",
       " 'knowledge': 429,\n",
       " 'level': 430,\n",
       " 'move': 431,\n",
       " 'operations': 432,\n",
       " 'opportunity': 433,\n",
       " 'resilient': 434,\n",
       " 'set': 435,\n",
       " 'skills': 436,\n",
       " 'success': 437,\n",
       " 'calculate': 438,\n",
       " 'descriptive': 439,\n",
       " 'insurance': 440,\n",
       " 'statistical': 441,\n",
       " 'statistics': 442,\n",
       " 'available': 443,\n",
       " 'catalog': 444,\n",
       " 'cloud‚ñ¢': 445,\n",
       " 'practice': 446,\n",
       " 'put': 447,\n",
       " 'sharing': 448,\n",
       " 'together': 449,\n",
       " 'advanced': 450,\n",
       " 'analytics‚äù': 451,\n",
       " 'associated': 452,\n",
       " 'came': 453,\n",
       " 'class': 454,\n",
       " 'core': 455,\n",
       " 'courses': 456,\n",
       " 'dataframe': 457,\n",
       " 'dataframes': 458,\n",
       " 'ecosystem': 459,\n",
       " 'establish': 460,\n",
       " 'finished': 461,\n",
       " 'general': 462,\n",
       " 'graph': 463,\n",
       " 'history': 464,\n",
       " 'leverage': 465,\n",
       " 'meant': 466,\n",
       " 'modules': 467,\n",
       " 'overview': 468,\n",
       " 'prepared': 469,\n",
       " 'rdd': 470,\n",
       " 'rdds': 471,\n",
       " 'recommend': 472,\n",
       " 'spark‚ñ¢': 473,\n",
       " 'student': 474,\n",
       " 'students': 475,\n",
       " 'topics': 476,\n",
       " '‚': 477,\n",
       " 'ee': 478,\n",
       " 'jakarta': 479,\n",
       " 'microshed': 480,\n",
       " 'activities': 481,\n",
       " 'beautiful': 482,\n",
       " 'becoming': 483,\n",
       " 'completing': 484,\n",
       " 'every': 485,\n",
       " 'factors': 486,\n",
       " 'frames': 487,\n",
       " 'free': 488,\n",
       " 'gained': 489,\n",
       " 'grows': 490,\n",
       " 'increasing': 491,\n",
       " 'leading': 492,\n",
       " 'lists': 493,\n",
       " 'million': 494,\n",
       " 'number': 495,\n",
       " 'online': 496,\n",
       " 'organizations': 497,\n",
       " 'ready': 498,\n",
       " 'today': 499,\n",
       " 'undertake': 500,\n",
       " 'worldwide': 501,\n",
       " 'year': 502,\n",
       " 'text': 503,\n",
       " 'cool': 504,\n",
       " 'expose': 505,\n",
       " 'full': 506,\n",
       " 'make': 507,\n",
       " 'mobile': 508,\n",
       " 'monetize': 509,\n",
       " 'side': 510,\n",
       " 'assessment': 511,\n",
       " 'carlo': 512,\n",
       " 'method': 513,\n",
       " 'monte': 514,\n",
       " 'montecarlo': 515,\n",
       " 'probability': 516,\n",
       " 'risk': 517,\n",
       " 'ruin': 518,\n",
       " 'anlysis': 519,\n",
       " 'teaching': 520,\n",
       " 'convert': 521,\n",
       " 'emotion': 522,\n",
       " 'ios': 523,\n",
       " 'photo': 524,\n",
       " 'sentiment': 525,\n",
       " 'speech': 526,\n",
       " 'three': 527,\n",
       " 'various': 528,\n",
       " 'add': 529,\n",
       " 'animal': 530,\n",
       " 'barking': 531,\n",
       " 'cat': 532,\n",
       " 'dog': 533,\n",
       " 'experience': 534,\n",
       " 'identify': 535,\n",
       " 'image': 536,\n",
       " 'integrate': 537,\n",
       " 'making': 538,\n",
       " 'page': 539,\n",
       " 'purring': 540,\n",
       " 'recognition': 541,\n",
       " 'specific': 542,\n",
       " 'visual': 543,\n",
       " 'communicate': 544,\n",
       " 'fundamental': 545,\n",
       " 'job': 546,\n",
       " 'mapreduce': 547,\n",
       " 'random': 548,\n",
       " 'real': 549,\n",
       " 'writes': 550,\n",
       " 'welcome': 551,\n",
       " 'actuarial': 552,\n",
       " 'business': 553,\n",
       " 'calculations': 554,\n",
       " 'mathematical': 555,\n",
       " 'model': 556,\n",
       " 'modelling': 557,\n",
       " 'process': 558,\n",
       " 'revenue': 559,\n",
       " 'accessing': 560,\n",
       " 'hive': 561,\n",
       " 'projects': 562,\n",
       " 'warehousing': 563,\n",
       " 'health': 564,\n",
       " 'istio': 565,\n",
       " 'kubernetes': 566,\n",
       " 'managing': 567,\n",
       " 'mesh': 568,\n",
       " 'observe': 569,\n",
       " 'secure': 570,\n",
       " 'shows': 571,\n",
       " 'traffic': 572,\n",
       " 'apply': 573,\n",
       " 'capable': 574,\n",
       " 'capture': 575,\n",
       " 'discovering': 576,\n",
       " 'hidden': 577,\n",
       " 'instance': 578,\n",
       " 'kind': 579,\n",
       " 'library': 580,\n",
       " 'majority': 581,\n",
       " 'relevant': 582,\n",
       " 'shallow': 583,\n",
       " 'solve': 584,\n",
       " 'structure': 585,\n",
       " 'structures': 586,\n",
       " 'unlabeled': 587,\n",
       " 'unstructured': 588,\n",
       " 'within¬†these': 589,\n",
       " 'applicable': 590,\n",
       " 'blogs': 591,\n",
       " 'brand': 592,\n",
       " 'call': 593,\n",
       " 'comments': 594,\n",
       " 'competitors': 595,\n",
       " 'constitutes': 596,\n",
       " 'customers': 597,\n",
       " 'emails': 598,\n",
       " 'employees': 599,\n",
       " 'example': 600,\n",
       " 'face': 601,\n",
       " 'find': 602,\n",
       " 'forms': 603,\n",
       " 'forums': 604,\n",
       " 'industries': 605,\n",
       " 'leaked': 606,\n",
       " 'like': 607,\n",
       " 'measure': 608,\n",
       " 'millions': 609,\n",
       " 'negative': 610,\n",
       " 'pain': 611,\n",
       " 'perceptions': 612,\n",
       " 'points': 613,\n",
       " 'positive': 614,\n",
       " 'product': 615,\n",
       " 'products': 616,\n",
       " 'questions': 617,\n",
       " 'secrets': 618,\n",
       " 'suspicious': 619,\n",
       " 'tweets': 620,\n",
       " 'accurate': 621,\n",
       " 'addressed': 622,\n",
       " 'cascading': 623,\n",
       " 'continuation': 624,\n",
       " 'declarative': 625,\n",
       " 'detail': 626,\n",
       " 'domain': 627,\n",
       " 'early': 628,\n",
       " 'enhance': 629,\n",
       " 'explains': 630,\n",
       " 'expressivity': 631,\n",
       " 'extraction': 632,\n",
       " 'extractors': 633,\n",
       " 'formalism': 634,\n",
       " 'grammars': 635,\n",
       " 'information': 636,\n",
       " 'limitations': 637,\n",
       " 'maintain': 638,\n",
       " 'new': 639,\n",
       " 'principles': 640,\n",
       " 'resulting': 641,\n",
       " 'results': 642,\n",
       " 'runtime': 643,\n",
       " 'scalable': 644,\n",
       " 'standard': 645,\n",
       " 'suffer': 646,\n",
       " 'systemt': 647,\n",
       " 'algebra': 648,\n",
       " 'automatic': 649,\n",
       " 'constructs': 650,\n",
       " 'expressed': 651,\n",
       " 'generation': 652,\n",
       " 'includes': 653,\n",
       " 'linear': 654,\n",
       " 'ml': 655,\n",
       " 'optimized': 656,\n",
       " 'plans': 657,\n",
       " 'primitives': 658,\n",
       " 'ranging': 659,\n",
       " 'single': 660,\n",
       " 'style': 661,\n",
       " 'systemml': 662,\n",
       " 'action': 663,\n",
       " 'firewall': 664,\n",
       " 'internet': 665,\n",
       " 'logs': 666,\n",
       " 'task': 667,\n",
       " 'orchestration': 668,\n",
       " 'care': 669,\n",
       " 'covid': 670,\n",
       " 'dynamics': 671,\n",
       " 'prognostication': 672,\n",
       " 'spread': 673,\n",
       " 'areas': 674,\n",
       " 'collection': 675,\n",
       " 'frameworks': 676,\n",
       " 'helping': 677,\n",
       " 'junction': 678,\n",
       " 'sms': 679,\n",
       " 'spam': 680,\n",
       " 'banking': 681,\n",
       " 'area': 682,\n",
       " 'card': 683,\n",
       " 'clients': 684,\n",
       " 'credit': 685,\n",
       " 'preliminary': 686,\n",
       " 'anomaly': 687,\n",
       " 'detection': 688,\n",
       " 'evaluation': 689,\n",
       " 'intrusion': 690,\n",
       " 'iscxids2012': 691,\n",
       " 'nlp': 692,\n",
       " 'backend': 693,\n",
       " 'frontend': 694,\n",
       " 'heard': 695,\n",
       " 'js': 696,\n",
       " 'know': 697,\n",
       " 'technology': 698,\n",
       " 'backup': 699,\n",
       " 'cli': 700,\n",
       " 'command': 701,\n",
       " 'contents': 702,\n",
       " 'dump': 703,\n",
       " 'explore': 704,\n",
       " 'restore': 705,\n",
       " 'tables': 706,\n",
       " 'excited': 707,\n",
       " 'features': 708,\n",
       " 'functionality': 709,\n",
       " 'implement': 710,\n",
       " 'satellite': 711,\n",
       " 'quantum': 712,\n",
       " 'strange': 713,\n",
       " 'app': 714,\n",
       " 'debate': 715,\n",
       " 'detector': 716,\n",
       " 'dollar': 717,\n",
       " 'great': 718,\n",
       " 'hotdog': 719,\n",
       " 'idea': 720,\n",
       " 'land': 721,\n",
       " 'launch': 722,\n",
       " 'let': 723,\n",
       " 'next': 724,\n",
       " 'picture': 725,\n",
       " 'prove': 726,\n",
       " 'pursue': 727,\n",
       " 'settle': 728,\n",
       " 'tells': 729,\n",
       " 'wondered': 730,\n",
       " 'wonder‚': 731,\n",
       " 'you‚': 732,\n",
       " 'eda': 733,\n",
       " 'exploratory': 734,\n",
       " 'pandas': 735,\n",
       " 'articles': 736,\n",
       " 'derive': 737,\n",
       " 'enroll': 738,\n",
       " 'experts': 739,\n",
       " 'hot': 740,\n",
       " 'insights': 741,\n",
       " 'interested': 742,\n",
       " 'news': 743,\n",
       " 'valuable': 744,\n",
       " 'graphical': 745,\n",
       " 'gui': 746,\n",
       " 'phpmyadmin': 747,\n",
       " 'attribute': 748,\n",
       " 'cardinality': 749,\n",
       " 'degree': 750,\n",
       " 'entity': 751,\n",
       " 'learned': 752,\n",
       " 'relation': 753,\n",
       " 'terms': 754,\n",
       " 'achievement': 755,\n",
       " 'certificate': 756,\n",
       " 'completed': 757,\n",
       " 'download': 758,\n",
       " 'labs': 759,\n",
       " 'many': 760,\n",
       " 'modeler': 761,\n",
       " 'modeling': 762,\n",
       " 'predictive': 763,\n",
       " 'print': 764,\n",
       " 'spss': 765,\n",
       " 'trial': 766,\n",
       " 'wish': 767,\n",
       " 'asset': 768,\n",
       " 'blockchain': 769,\n",
       " 'chain': 770,\n",
       " 'configure': 771,\n",
       " 'dashboard': 772,\n",
       " 'device': 773,\n",
       " 'monitor': 774,\n",
       " 'perishable': 775,\n",
       " 'purchase': 776,\n",
       " 'simulated': 777,\n",
       " 'supply': 778,\n",
       " 'tracker': 779,\n",
       " 'tracking': 780,\n",
       " 'chaincode': 781,\n",
       " 'developing': 782,\n",
       " 'postgresql': 783,\n",
       " 'components': 784,\n",
       " 'composer': 785,\n",
       " 'consensus': 786,\n",
       " 'contracts': 787,\n",
       " 'deeper': 788,\n",
       " 'foundation': 789,\n",
       " 'hyperledger': 790,\n",
       " 'ledgers': 791,\n",
       " 'enforce': 792,\n",
       " 'entry': 793,\n",
       " 'keys': 794,\n",
       " 'relationships': 795,\n",
       " 'rules': 796,\n",
       " 'addition': 797,\n",
       " 'allows': 798,\n",
       " 'builders': 799,\n",
       " 'computation': 800,\n",
       " 'edges': 801,\n",
       " 'exploring': 802,\n",
       " 'graphx': 803,\n",
       " 'growing': 804,\n",
       " 'operators': 805,\n",
       " 'paradigm': 806,\n",
       " 'parallel': 807,\n",
       " 'representation': 808,\n",
       " 'simplify': 809,\n",
       " 'tasks': 810,\n",
       " 'vertices': 811,\n",
       " 'blood': 812,\n",
       " 'collected': 813,\n",
       " 'diseases': 814,\n",
       " 'elisa': 815,\n",
       " 'groups': 816,\n",
       " 'igg': 817,\n",
       " 'igm': 818,\n",
       " 'influenza': 819,\n",
       " 'pre': 820,\n",
       " 'preparate': 821,\n",
       " 'previous': 822,\n",
       " 'tuberculosis': 823,\n",
       " 'vaccination': 824,\n",
       " 'pgadmin': 825,\n",
       " 'metrics': 826,\n",
       " 'monitoring': 827,\n",
       " 'provide': 828,\n",
       " 'config': 829,\n",
       " 'configmaps': 830,\n",
       " 'configuration': 831,\n",
       " 'configuring': 832,\n",
       " 'externalize': 833,\n",
       " 'agents': 834,\n",
       " 'cartpole': 835,\n",
       " 'games': 836,\n",
       " 'going': 837,\n",
       " 'play': 838,\n",
       " 'playing': 839,\n",
       " 's4tf': 840,\n",
       " 'tac': 841,\n",
       " 'tic': 842,\n",
       " 'toe': 843,\n",
       " 'code': 844,\n",
       " 'cors': 845,\n",
       " 'cross': 846,\n",
       " 'origin': 847,\n",
       " 'resource': 848,\n",
       " 'writing': 849,\n",
       " 'actual': 850,\n",
       " 'allow': 851,\n",
       " 'anything': 852,\n",
       " 'data‚': 853,\n",
       " 'developed': 854,\n",
       " 'eat': 855,\n",
       " 'hence': 856,\n",
       " 'mapper': 857,\n",
       " 'name': 858,\n",
       " 'people': 859,\n",
       " 'pig': 860,\n",
       " 'pigs': 861,\n",
       " 'programs': 862,\n",
       " 'reducer': 863,\n",
       " 'sets': 864,\n",
       " 'spend': 865,\n",
       " 'write': 866,\n",
       " 'yahoo': 867,\n",
       " 'biginsights': 868,\n",
       " 'contextualize': 869,\n",
       " 'examples': 870,\n",
       " 'manage': 871,\n",
       " 'motivate': 872,\n",
       " 'study': 873,\n",
       " 'tandem': 874,\n",
       " 'zookeeper': 875,\n",
       " 'controlling': 876,\n",
       " 'jobs': 877,\n",
       " 'materials': 878,\n",
       " 'oozie': 879,\n",
       " 'provided': 880,\n",
       " 'click': 881,\n",
       " 'commands': 882,\n",
       " 'covered': 883,\n",
       " 'describes': 884,\n",
       " 'flume': 885,\n",
       " 'moving': 886,\n",
       " 'presented': 887,\n",
       " 'shell': 888,\n",
       " 'sophisticated': 889,\n",
       " 'sqoop': 890,\n",
       " 'techniques': 891,\n",
       " 'variety': 892,\n",
       " 'ways': 893,\n",
       " 'exposure': 894,\n",
       " 'gaining': 895,\n",
       " 'mapreduce1': 896,\n",
       " 'negotiator': 897,\n",
       " 'start': 898,\n",
       " 'string': 899,\n",
       " 'yarn': 900,\n",
       " 'acknowledge': 901,\n",
       " 'acknowledging': 902,\n",
       " 'messages': 903,\n",
       " 'messaging': 904,\n",
       " 'choosing': 905,\n",
       " 'decision': 906,\n",
       " 'django': 907,\n",
       " 'evening': 908,\n",
       " 'handy': 909,\n",
       " 'movie': 910,\n",
       " 'personal': 911,\n",
       " 'recommender': 912,\n",
       " 'watch': 913,\n",
       " 'weekend': 914,\n",
       " 'cdi': 915,\n",
       " 'contexts': 916,\n",
       " 'dependency': 917,\n",
       " 'inject': 918,\n",
       " 'injecting': 919,\n",
       " 'injection': 920,\n",
       " 'scopes': 921,\n",
       " 'essential': 922,\n",
       " 'ignite': 923,\n",
       " 'interest': 924,\n",
       " 'processor': 925,\n",
       " 'deploying': 926,\n",
       " 'kubectl': 927,\n",
       " 'maven': 928,\n",
       " 'rest': 929,\n",
       " 'assistant': 930,\n",
       " 'brings': 931,\n",
       " 'challenging': 932,\n",
       " 'correlation': 933,\n",
       " 'ends': 934,\n",
       " 'evaluations': 935,\n",
       " 'exercise': 936,\n",
       " 'fail': 937,\n",
       " 'gentle': 938,\n",
       " 'haider': 939,\n",
       " 'hot‚äù': 940,\n",
       " 'included': 941,\n",
       " 'looking': 942,\n",
       " 'murtaza': 943,\n",
       " 'professor': 944,\n",
       " 'ryerson': 945,\n",
       " 'taught': 946,\n",
       " 'variance': 947,\n",
       " 'visualization': 948,\n",
       " 'ability': 949,\n",
       " 'able': 950,\n",
       " 'analyst': 951,\n",
       " 'bigsheets': 952,\n",
       " 'capabilities': 953,\n",
       " 'component': 954,\n",
       " 'spreadsheet': 955,\n",
       " 'type': 956,\n",
       " 'ui': 957,\n",
       " 'visualize': 958,\n",
       " 'consultant': 959,\n",
       " 'less': 960,\n",
       " 'openrefine': 961,\n",
       " 'technical': 962,\n",
       " 'solr': 963,\n",
       " 'annotation': 964,\n",
       " 'behaviours': 965,\n",
       " 'failures': 966,\n",
       " 'fallback': 967,\n",
       " 'fault': 968,\n",
       " 'impact': 969,\n",
       " 'retry': 970,\n",
       " 'tolerance': 971,\n",
       " 'tolerant': 972,\n",
       " 'interfaces': 973,\n",
       " 'safe': 974,\n",
       " 'template': 975,\n",
       " 'chatbot': 976,\n",
       " 'chatbots': 977,\n",
       " 'complete': 978,\n",
       " 'conversation': 979,\n",
       " 'conversational': 980,\n",
       " 'culminating': 981,\n",
       " 'depth': 982,\n",
       " 'functional': 983,\n",
       " 'seven': 984,\n",
       " 'bots': 985,\n",
       " 'cognitive': 986,\n",
       " 'facebook': 987,\n",
       " 'messenger': 988,\n",
       " 'send': 989,\n",
       " 'tone': 990,\n",
       " 'translate': 991,\n",
       " 'inconsistency': 992,\n",
       " 'integrity': 993,\n",
       " 'kinds': 994,\n",
       " 'minimize': 995,\n",
       " 'normalization': 996,\n",
       " 'normalizing': 997,\n",
       " 'record': 998,\n",
       " 'redundancy': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "tokens_dict = gensim.corpora.Dictionary(tokenized_courses)\n",
    "tokens_dict.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can use `doc2bow()` method to generate BoW features for each tokenized course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Use tokens_dict.doc2bow() to generate BoW features for each tokenized course.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 2),\n",
       "  (1, 2),\n",
       "  (2, 2),\n",
       "  (3, 1),\n",
       "  (4, 1),\n",
       "  (5, 1),\n",
       "  (6, 1),\n",
       "  (7, 1),\n",
       "  (8, 1),\n",
       "  (9, 1),\n",
       "  (10, 4),\n",
       "  (11, 1),\n",
       "  (12, 2),\n",
       "  (13, 1),\n",
       "  (14, 1),\n",
       "  (15, 1),\n",
       "  (16, 2),\n",
       "  (17, 1),\n",
       "  (18, 1),\n",
       "  (19, 1),\n",
       "  (20, 1),\n",
       "  (21, 1),\n",
       "  (22, 1),\n",
       "  (23, 2),\n",
       "  (24, 1),\n",
       "  (25, 1),\n",
       "  (26, 1),\n",
       "  (27, 2),\n",
       "  (28, 1),\n",
       "  (29, 1),\n",
       "  (30, 1),\n",
       "  (31, 1),\n",
       "  (32, 2),\n",
       "  (33, 1)]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "courses_bow = [tokens_dict.doc2bow(course) for course in tokenized_courses]\n",
    "courses_bow[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, you need to append the BoW features for each course into a new BoW dataframe. The new dataframe needs to include the following columns (you may include other relevant columns as well):\n",
    "\n",
    "*   'doc_index': the course index starting from 0\n",
    "*   'doc_id': the actual course id such as `ML0201EN`\n",
    "*   'token': the tokens for each course\n",
    "*   'bow': the bow value for each token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*TODO: Create a new course_bow dataframe based on the extracted BoW features.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>COURSE_ID</th>\n",
       "      <th>TITLE</th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>course_texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>robots are coming  build iot apps with watson ...</td>\n",
       "      <td>have fun with iot and learn along the way  if ...</td>\n",
       "      <td>robots are coming  build iot apps with watson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ML0122EN</td>\n",
       "      <td>accelerating deep learning with gpu</td>\n",
       "      <td>training complex deep learning models with lar...</td>\n",
       "      <td>accelerating deep learning with gpu training c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>GPXX0ZG0EN</td>\n",
       "      <td>consuming restful services using the reactive ...</td>\n",
       "      <td>learn how to use a reactive jax rs client to a...</td>\n",
       "      <td>consuming restful services using the reactive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>RP0105EN</td>\n",
       "      <td>analyzing big data in r using apache spark</td>\n",
       "      <td>apache spark is a popular cluster computing fr...</td>\n",
       "      <td>analyzing big data in r using apache spark apa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>GPXX0Z2PEN</td>\n",
       "      <td>containerizing  packaging  and running a sprin...</td>\n",
       "      <td>learn how to containerize  package  and run a ...</td>\n",
       "      <td>containerizing  packaging  and running a sprin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>302</td>\n",
       "      <td>excourse89</td>\n",
       "      <td>javascript  jquery  and json</td>\n",
       "      <td>in this course  we ll look at the javascript l...</td>\n",
       "      <td>javascript  jquery  and json in this course  w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>303</td>\n",
       "      <td>excourse90</td>\n",
       "      <td>programming foundations with javascript  html ...</td>\n",
       "      <td>learn foundational programming concepts  e g  ...</td>\n",
       "      <td>programming foundations with javascript  html ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>304</td>\n",
       "      <td>excourse91</td>\n",
       "      <td>front end web development with react</td>\n",
       "      <td>this course explores javascript based front en...</td>\n",
       "      <td>front end web development with react this cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>305</td>\n",
       "      <td>excourse92</td>\n",
       "      <td>introduction to web development</td>\n",
       "      <td>this course is designed to start you on a path...</td>\n",
       "      <td>introduction to web development this course is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>306</td>\n",
       "      <td>excourse93</td>\n",
       "      <td>interactivity with javascript and jquery</td>\n",
       "      <td>this course is the third in our javascript for...</td>\n",
       "      <td>interactivity with javascript and jquery this ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index   COURSE_ID                                              TITLE  \\\n",
       "0        0    ML0201EN  robots are coming  build iot apps with watson ...   \n",
       "1        1    ML0122EN                accelerating deep learning with gpu   \n",
       "2        2  GPXX0ZG0EN  consuming restful services using the reactive ...   \n",
       "3        3    RP0105EN         analyzing big data in r using apache spark   \n",
       "4        4  GPXX0Z2PEN  containerizing  packaging  and running a sprin...   \n",
       "..     ...         ...                                                ...   \n",
       "302    302  excourse89                       javascript  jquery  and json   \n",
       "303    303  excourse90  programming foundations with javascript  html ...   \n",
       "304    304  excourse91               front end web development with react   \n",
       "305    305  excourse92                    introduction to web development   \n",
       "306    306  excourse93           interactivity with javascript and jquery   \n",
       "\n",
       "                                           DESCRIPTION  \\\n",
       "0    have fun with iot and learn along the way  if ...   \n",
       "1    training complex deep learning models with lar...   \n",
       "2    learn how to use a reactive jax rs client to a...   \n",
       "3    apache spark is a popular cluster computing fr...   \n",
       "4    learn how to containerize  package  and run a ...   \n",
       "..                                                 ...   \n",
       "302  in this course  we ll look at the javascript l...   \n",
       "303  learn foundational programming concepts  e g  ...   \n",
       "304  this course explores javascript based front en...   \n",
       "305  this course is designed to start you on a path...   \n",
       "306  this course is the third in our javascript for...   \n",
       "\n",
       "                                          course_texts  \n",
       "0    robots are coming  build iot apps with watson ...  \n",
       "1    accelerating deep learning with gpu training c...  \n",
       "2    consuming restful services using the reactive ...  \n",
       "3    analyzing big data in r using apache spark apa...  \n",
       "4    containerizing  packaging  and running a sprin...  \n",
       "..                                                 ...  \n",
       "302  javascript  jquery  and json in this course  w...  \n",
       "303  programming foundations with javascript  html ...  \n",
       "304  front end web development with react this cour...  \n",
       "305  introduction to web development this course is...  \n",
       "306  interactivity with javascript and jquery this ...  \n",
       "\n",
       "[307 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course_content_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>doc_id</th>\n",
       "      <th>token</th>\n",
       "      <th>bow</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>ai</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>apps</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>build</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>cloud</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>ML0201EN</td>\n",
       "      <td>coming</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10358</th>\n",
       "      <td>306</td>\n",
       "      <td>excourse93</td>\n",
       "      <td>modifying</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10359</th>\n",
       "      <td>306</td>\n",
       "      <td>excourse93</td>\n",
       "      <td>objectives</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10360</th>\n",
       "      <td>306</td>\n",
       "      <td>excourse93</td>\n",
       "      <td>pieces</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10361</th>\n",
       "      <td>306</td>\n",
       "      <td>excourse93</td>\n",
       "      <td>plugins</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10362</th>\n",
       "      <td>306</td>\n",
       "      <td>excourse93</td>\n",
       "      <td>populate</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10363 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       doc_index      doc_id       token  bow\n",
       "0              0    ML0201EN          ai    2\n",
       "1              0    ML0201EN        apps    2\n",
       "2              0    ML0201EN       build    2\n",
       "3              0    ML0201EN       cloud    1\n",
       "4              0    ML0201EN      coming    1\n",
       "...          ...         ...         ...  ...\n",
       "10358        306  excourse93   modifying    1\n",
       "10359        306  excourse93  objectives    1\n",
       "10360        306  excourse93      pieces    1\n",
       "10361        306  excourse93     plugins    1\n",
       "10362        306  excourse93    populate    1\n",
       "\n",
       "[10363 rows x 4 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WRITE YOUR CODE HERE\n",
    "doc_index = []\n",
    "doc_id = []\n",
    "bags_of_token = []\n",
    "bow = []\n",
    "\n",
    "for idx, bag in enumerate(courses_bow):\n",
    "    for word in bag:\n",
    "        token = tokens_dict[word[0]]\n",
    "        doc_index.append(idx)\n",
    "        doc_id.append(course_content_df['COURSE_ID'][idx])\n",
    "        bags_of_token.append(token)\n",
    "        bow.append(word[1])\n",
    "\n",
    "\n",
    "bow_dicts = {\"doc_index\": doc_index,\n",
    "           \"doc_id\": doc_id,\n",
    "            \"token\": bags_of_token,\n",
    "            \"bow\": bow}\n",
    "pd.DataFrame(bow_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your course BoW dataframe may look like the following:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module\\_2/images/bow_dataset.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may refer to previous code examples in this lab if you need help with creating the BoW dataframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other popular textual features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the basic token BoW feature, there are two other types of widely used textual features. If you are interested, you may explore them yourself to learn how to extract them from the course textual content:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **tf-idf**: tf-idf refers to Term Frequency–Inverse Document Frequency. Similar to BoW, the tf-idf also counts the word frequencies in each document. Furthermore, tf-idf will  offset the number of documents in the corpus that contain the word in order to adjust for the fact that some words appear more frequently in general. The higher the tf-idf normally means the greater the importance the word/token is.\n",
    "*   **Text embedding vector**. Embedding means projecting an object into a latent feature space. We normally employ neural networks or deep neural networks to learn the latent features of a textual object such as a word, a sentence, or the entire document. The learned latent feature vectors will be used to represent the original textual entities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have completed the BoW feature extraction lab. In this lab, you have learned and practiced extracting BoW features from course titles and descriptions. Once the feature vectors on the courses has been built, we can then apply machine learning algorithms such as similarity measurements, clustering, or classification on the courses in later labs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork32585014-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Contributors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description          |\n",
    "| ----------------- | ------- | ---------- | --------------------------- |\n",
    "| 2021-10-25        | 1.0     | Yan        | Created the initial version |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2021 IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
